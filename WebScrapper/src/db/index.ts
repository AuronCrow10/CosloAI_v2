import { Pool } from 'pg';
import {
  ChunkWithEmbedding,
  Client,
  DbConfig,
  SearchResult,
} from '../types.js';
import { logger } from '../logger.js';
import {
  EmbeddingModel,
  getModelDimensions,
} from '../embeddings/models.js';

export class Database {
  private pool: Pool;

  constructor(config: DbConfig) {
    this.pool = new Pool({
      connectionString: config.connectionString,
    });
  }

  async init(): Promise<void> {
    const client = await this.pool.connect();
    try {
      // Check pgvector extension
      const extRes = await client.query(
        `SELECT 1 FROM pg_extension WHERE extname = 'vector'`,
      );
      if (extRes.rowCount === 0) {
        throw new Error(
          "pgvector extension is not enabled. Run `CREATE EXTENSION IF NOT EXISTS vector;`",
        );
      }

      // Check required tables exist
      const tablesRes = await client.query<{
        clients: string | null;
        small: string | null;
        large: string | null;
      }>(`
        SELECT
          to_regclass('public.clients') AS clients,
          to_regclass('public.page_chunks_small') AS small,
          to_regclass('public.page_chunks_large') AS large
      `);

      const row = tablesRes.rows[0];
      if (!row || !row.clients || !row.small || !row.large) {
        throw new Error(
          'Required tables (clients, page_chunks_small, page_chunks_large) are missing. Run migrations.',
        );
      }

      logger.info('Connected to PostgreSQL and basic schema checks passed');
    } catch (err) {
      logger.error('Failed to initialize database', err);
      throw err;
    } finally {
      client.release();
    }
  }

  async close(): Promise<void> {
    await this.pool.end();
  }

  // ---------- CLIENTS ----------

  /**
   * Fetch a client by ID, validating the embedding_model value.
   */
  async getClientById(id: string): Promise<Client | null> {
    const client = await this.pool.connect();
    try {
      const res = await client.query<{
        id: string;
        name: string;
        embedding_model: string;
        main_domain: string | null;
        created_at: Date;
      }>(
        `
        SELECT id, name, embedding_model, main_domain, created_at
        FROM clients
        WHERE id = $1
        `,
        [id],
      );

      if (res.rowCount === 0) {
        return null;
      }

      const row = res.rows[0];
      if (
        row.embedding_model !== 'text-embedding-3-small' &&
        row.embedding_model !== 'text-embedding-3-large'
      ) {
        throw new Error(
          `Client ${row.id} has unsupported embedding_model="${row.embedding_model}". ` +
            `Allowed values: "text-embedding-3-small", "text-embedding-3-large".`,
        );
      }

      const clientObj: Client = {
        id: row.id,
        name: row.name,
        embeddingModel: row.embedding_model as EmbeddingModel,
        mainDomain: row.main_domain,
        createdAt: row.created_at,
      };

      return clientObj;
    } finally {
      client.release();
    }
  }

  /**
   * Optionally: fetch client by main_domain, if you store it.
   */
  async getClientByMainDomain(domain: string): Promise<Client | null> {
    const client = await this.pool.connect();
    try {
      const res = await client.query<{
        id: string;
        name: string;
        embedding_model: string;
        main_domain: string | null;
        created_at: Date;
      }>(
        `
        SELECT id, name, embedding_model, main_domain, created_at
        FROM clients
        WHERE main_domain = $1
        `,
        [domain],
      );

      if (res.rowCount === 0) return null;

      const row = res.rows[0];
      if (
        row.embedding_model !== 'text-embedding-3-small' &&
        row.embedding_model !== 'text-embedding-3-large'
      ) {
        throw new Error(
          `Client ${row.id} has unsupported embedding_model="${row.embedding_model}".`,
        );
      }

      return {
        id: row.id,
        name: row.name,
        embeddingModel: row.embedding_model as EmbeddingModel,
        mainDomain: row.main_domain,
        createdAt: row.created_at,
      };
    } finally {
      client.release();
    }
  }

  /**
   * Create a new client. The ID is generated by Postgres via gen_random_uuid().
   * If main_domain is already used, this will throw an error with code 'DUPLICATE_MAIN_DOMAIN'.
   */
  async createClient(params: {
    name: string;
    embeddingModel: EmbeddingModel;
    mainDomain?: string | null;
  }): Promise<Client> {
    const client = await this.pool.connect();
    try {
      const res = await client.query<{
        id: string;
        name: string;
        embedding_model: string;
        main_domain: string | null;
        created_at: Date;
      }>(
        `
        INSERT INTO clients (name, embedding_model, main_domain)
        VALUES ($1, $2, $3)
        RETURNING id, name, embedding_model, main_domain, created_at
        `,
        [params.name, params.embeddingModel, params.mainDomain ?? null],
      );

      const row = res.rows[0];

      if (
        row.embedding_model !== 'text-embedding-3-small' &&
        row.embedding_model !== 'text-embedding-3-large'
      ) {
        throw new Error(
          `Client ${row.id} has unsupported embedding_model="${row.embedding_model}".`,
        );
      }

      return {
        id: row.id,
        name: row.name,
        embeddingModel: row.embedding_model as EmbeddingModel,
        mainDomain: row.main_domain,
        createdAt: row.created_at,
      };
    } catch (err: any) {
      // 23505 = unique_violation (likely idx_clients_main_domain_unique)
      if (err?.code === '23505') {
        const e = new Error('DUPLICATE_MAIN_DOMAIN');
        (e as any).code = 'DUPLICATE_MAIN_DOMAIN';
        throw e;
      }
      throw err;
    } finally {
      client.release();
    }
  }


  /**
   * Delete a client by ID.
   * Thanks to ON DELETE CASCADE, this also removes all page_chunks_* and client_usage rows.
   */
  async deleteClientById(id: string): Promise<void> {
    const client = await this.pool.connect();
    try {
      await client.query(
        `
        DELETE FROM clients
        WHERE id = $1
        `,
        [id],
      );
    } finally {
      client.release();
    }
  }

  // ---------- PAGE CHUNKS (SMALL / LARGE) ----------

  private getTableForModel(model: EmbeddingModel): {
    tableName: string;
    dims: number;
  } {
    const dims = getModelDimensions(model);
    const tableName =
      model === 'text-embedding-3-small'
        ? 'page_chunks_small'
        : 'page_chunks_large';
    return { tableName, dims };
  }

  /**
   * Insert a chunk + embedding for a given client and model.
   * - Routes to the correct table (small/large).
   * - Validates embedding dimension.
   * - Uses per-client dedup via (client_id, chunk_hash) UNIQUE.
   */
  async insertChunkForClient(
    clientId: string,
    model: EmbeddingModel,
    chunk: ChunkWithEmbedding,
  ): Promise<void> {
    const { tableName, dims } = this.getTableForModel(model);

    if (chunk.embedding.length !== dims) {
      throw new Error(
        `Embedding dimension mismatch: got ${chunk.embedding.length}, expected ${dims} for model "${model}".`,
      );
    }

    const client = await this.pool.connect();
    try {
      const embeddingLiteral = `[${chunk.embedding.join(',')}]`;

      const result = await client.query(
        `
        INSERT INTO ${tableName} (
          id,
          client_id,
          domain,
          url,
          chunk_index,
          chunk_text,
          chunk_hash,
          embedding
        )
        VALUES (
          gen_random_uuid(),
          $1, $2, $3, $4, $5, $6, $7::vector
        )
        ON CONFLICT (client_id, chunk_hash) DO NOTHING
        RETURNING id
        `,
        [
          clientId,
          chunk.domain,
          chunk.url,
          chunk.chunkIndex,
          chunk.text,
          chunk.chunkHash,
          embeddingLiteral,
        ],
      );

      if (result.rowCount === 0) {
        // Duplicate detected via per-client hash
        logger.debug(
          `Duplicate chunk for client ${clientId} (hash=${chunk.chunkHash}) detected in ${tableName}, skipping insert.`,
        );
      }
    } catch (err) {
      logger.error('Failed to insert chunk into DB', err);
      throw err;
    } finally {
      client.release();
    }
  }

  /**
   * Semantic search by client & model, optionally filtered by domain.
   *
   * For text-embedding-3-small (1536 dims):
   *   - query against page_chunks_small
   *   - use embedding <-> $N::vector
   *   - ivfflat index on vector(1536)
   *
   * For text-embedding-3-large (3072 dims):
   *   - query against page_chunks_large
   *   - use (embedding::halfvec(3072)) <-> $N::halfvec
   *   - HNSW index on (embedding::halfvec(3072))
   */
  async searchClientChunks(params: {
    clientId: string;
    model: EmbeddingModel;
    queryEmbedding: number[];
    domain?: string;
    limit: number;
  }): Promise<SearchResult[]> {
    const { clientId, model, queryEmbedding, domain, limit } = params;
    const { tableName, dims } = this.getTableForModel(model);

    // Sanity check sulla dimensione dell'embedding
    if (queryEmbedding.length !== dims) {
      throw new Error(
        `Query embedding dimension mismatch: got ${queryEmbedding.length}, expected ${dims} for model "${model}".`,
      );
    }

    // pgvector accetta il formato testuale "[x,y,z,...]" per vector/halfvec
    const embeddingLiteral = `[${queryEmbedding.join(',')}]`;
    const isLarge = model === 'text-embedding-3-large';

    const client = await this.pool.connect();
    try {
      let sql: string;
      let values: unknown[];

      if (domain) {
        // ðŸ”¹ Con filtro per domain
        if (isLarge) {
          // LARGE MODEL: usa halfvec per sfruttare l'indice ANN su (embedding::halfvec)
          sql = `
          SELECT
            id,
            client_id,
            domain,
            url,
            chunk_index,
            chunk_text,
            created_at,
            (embedding::halfvec) <-> $3::halfvec AS distance
          FROM ${tableName}
          WHERE client_id = $1
            AND domain = $2
          ORDER BY (embedding::halfvec) <-> $3::halfvec
          LIMIT $4
        `;
          values = [clientId, domain, embeddingLiteral, limit];
        } else {
          // SMALL MODEL: ricerca vettoriale normale su vector
          sql = `
          SELECT
            id,
            client_id,
            domain,
            url,
            chunk_index,
            chunk_text,
            created_at,
            embedding <-> $3::vector AS distance
          FROM ${tableName}
          WHERE client_id = $1
            AND domain = $2
          ORDER BY embedding <-> $3::vector
          LIMIT $4
        `;
          values = [clientId, domain, embeddingLiteral, limit];
        }
      } else {
        // ðŸ”¹ Senza filtro di domain
        if (isLarge) {
          sql = `
          SELECT
            id,
            client_id,
            domain,
            url,
            chunk_index,
            chunk_text,
            created_at,
            (embedding::halfvec) <-> $2::halfvec AS distance
          FROM ${tableName}
          WHERE client_id = $1
          ORDER BY (embedding::halfvec) <-> $2::halfvec
          LIMIT $3
        `;
          values = [clientId, embeddingLiteral, limit];
        } else {
          sql = `
          SELECT
            id,
            client_id,
            domain,
            url,
            chunk_index,
            chunk_text,
            created_at,
            embedding <-> $2::vector AS distance
          FROM ${tableName}
          WHERE client_id = $1
          ORDER BY embedding <-> $2::vector
          LIMIT $3
        `;
          values = [clientId, embeddingLiteral, limit];
        }
      }

      const res = await client.query<{
        id: string;
        client_id: string;
        domain: string;
        url: string;
        chunk_index: number;
        chunk_text: string;
        created_at: Date;
        distance: number;
      }>(sql, values);

      return res.rows.map((row) => {
        const distance = row.distance;
        // distanza bassa = piÃ¹ simile â†’ score alto
        const score = 1 / (1 + distance);

        const result: SearchResult = {
          id: row.id,
          clientId: row.client_id,
          domain: row.domain,
          url: row.url,
          chunkIndex: row.chunk_index,
          text: row.chunk_text,
          createdAt: row.created_at,
          score,
        };

        return result;
      });
    } finally {
      client.release();
    }
  }

  // ---------- USAGE TRACKING ----------

  /**
   * Record OpenAI token usage per client.
   * This is intentionally best-effort: errors are logged but do not break the main flow.
   */
  async recordUsage(params: {
    clientId: string;
    model: EmbeddingModel;
    operation: string; // e.g. 'embeddings_ingest', 'embeddings_search'
    promptTokens: number;
    totalTokens: number;
  }): Promise<void> {
    if (params.totalTokens <= 0 && params.promptTokens <= 0) {
      return;
    }

    const client = await this.pool.connect();
    try {
      await client.query(
        `
        INSERT INTO client_usage (
          client_id,
          model,
          operation,
          prompt_tokens,
          total_tokens
        )
        VALUES ($1, $2, $3, $4, $5)
        `,
        [
          params.clientId,
          params.model,
          params.operation,
          params.promptTokens,
          params.totalTokens,
        ],
      );
    } catch (err) {
      // Never break ingestion/search because of usage tracking
      logger.error('Failed to record usage', err);
    } finally {
      client.release();
    }
  }

  /**
   * Aggregate OpenAI token usage for a client.
   * If from/to are provided, restrict to that interval.
   * Returns:
   * - overall totals (prompt + total tokens)
   * - breakdown by model
   * - breakdown by operation
   */
  async getClientUsageSummary(
    clientId: string,
    from: Date | null,
    to: Date | null,
  ): Promise<{
    clientId: string;
    totalPromptTokens: number;
    totalTokens: number;
    byModel: { model: EmbeddingModel; promptTokens: number; totalTokens: number }[];
    byOperation: { operation: string; promptTokens: number; totalTokens: number }[];
  }> {
    const client = await this.pool.connect();
    try {
      // Overall totals
      const totalRes = await client.query<{
        prompt_tokens: string | null;
        total_tokens: string | null;
      }>(
        `
        SELECT
          COALESCE(SUM(prompt_tokens), 0) AS prompt_tokens,
          COALESCE(SUM(total_tokens), 0) AS total_tokens
        FROM client_usage
        WHERE client_id = $1
          AND ($2::timestamptz IS NULL OR created_at >= $2::timestamptz)
          AND ($3::timestamptz IS NULL OR created_at <= $3::timestamptz)
        `,
        [clientId, from ?? null, to ?? null],
      );

      const totalRow = totalRes.rows[0];
      const totalPromptTokens = Number(totalRow?.prompt_tokens ?? 0);
      const totalTokens = Number(totalRow?.total_tokens ?? 0);

      // Breakdown by model
      const byModelRes = await client.query<{
        model: EmbeddingModel;
        prompt_tokens: string;
        total_tokens: string;
      }>(
        `
        SELECT
          model,
          SUM(prompt_tokens) AS prompt_tokens,
          SUM(total_tokens) AS total_tokens
        FROM client_usage
        WHERE client_id = $1
          AND ($2::timestamptz IS NULL OR created_at >= $2::timestamptz)
          AND ($3::timestamptz IS NULL OR created_at <= $3::timestamptz)
        GROUP BY model
        ORDER BY model
        `,
        [clientId, from ?? null, to ?? null],
      );

      const byModel = byModelRes.rows.map((row) => ({
        model: row.model,
        promptTokens: Number(row.prompt_tokens),
        totalTokens: Number(row.total_tokens),
      }));

      // Breakdown by operation
      const byOperationRes = await client.query<{
        operation: string;
        prompt_tokens: string;
        total_tokens: string;
      }>(
        `
        SELECT
          operation,
          SUM(prompt_tokens) AS prompt_tokens,
          SUM(total_tokens) AS total_tokens
        FROM client_usage
        WHERE client_id = $1
          AND ($2::timestamptz IS NULL OR created_at >= $2::timestamptz)
          AND ($3::timestamptz IS NULL OR created_at <= $3::timestamptz)
        GROUP BY operation
        ORDER BY operation
        `,
        [clientId, from ?? null, to ?? null],
      );

      const byOperation = byOperationRes.rows.map((row) => ({
        operation: row.operation,
        promptTokens: Number(row.prompt_tokens),
        totalTokens: Number(row.total_tokens),
      }));

      return {
        clientId,
        totalPromptTokens,
        totalTokens,
        byModel,
        byOperation,
      };
    } finally {
      client.release();
    }
  }

  /**
   * Aggregate usage for all clients.
   * If from/to are provided, restrict to that interval.
   * Returns top N clients by total_tokens (desc).
   */
  async getAllClientsUsageSummary(
    limit: number,
    from: Date | null,
    to: Date | null,
  ): Promise<
    {
      clientId: string;
      name: string;
      totalPromptTokens: number;
      totalTokens: number;
    }[]
  > {
    const client = await this.pool.connect();
    try {
      const res = await client.query<{
        client_id: string;
        name: string;
        prompt_tokens: string | null;
        total_tokens: string | null;
      }>(
        `
        SELECT
          c.id AS client_id,
          c.name AS name,
          COALESCE(SUM(u.prompt_tokens), 0) AS prompt_tokens,
          COALESCE(SUM(u.total_tokens), 0) AS total_tokens
        FROM clients c
        LEFT JOIN client_usage u
          ON u.client_id = c.id
          AND ($2::timestamptz IS NULL OR u.created_at >= $2::timestamptz)
          AND ($3::timestamptz IS NULL OR u.created_at <= $3::timestamptz)
        GROUP BY c.id, c.name
        ORDER BY COALESCE(SUM(u.total_tokens), 0) DESC
        LIMIT $1
        `,
        [limit, from ?? null, to ?? null],
      );

      return res.rows.map((row) => ({
        clientId: row.client_id,
        name: row.name,
        totalPromptTokens: Number(row.prompt_tokens ?? 0),
        totalTokens: Number(row.total_tokens ?? 0),
      }));
    } finally {
      client.release();
    }
  }
}
